import os
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from wordcloud import WordCloud
import pandas as pd
import numpy as np
import re
import json
from collections import Counter
import matplotlib.pyplot as plt
import io
import requests
from datetime import datetime
import time

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ç”¨æˆ·ä¹‹å£°å›éŸ³å£ (Echo) Pro",
    page_icon="ğŸ”Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 0.5rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    .sub-header {
        font-size: 1.2rem;
        color: #666;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 1rem;
        text-align: center;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        transition: transform 0.3s ease;
    }
    .metric-card:hover {
        transform: translateY(-5px);
    }
    .insight-box {
        background: linear-gradient(135deg, #e3f2fd 0%, #f3e5f5 100%);
        padding: 1.5rem;
        border-radius: 1rem;
        border-left: 4px solid #1f77b4;
        margin: 1rem 0;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .positive-insight {
        background: linear-gradient(135deg, #e8f5e8 0%, #f1f8e9 100%);
        border-left: 4px solid #4caf50;
    }
    .negative-insight {
        background: linear-gradient(135deg, #ffebee 0%, #fce4ec 100%);
        border-left: 4px solid #f44336;
    }
    .feature-card {
        background: white;
        padding: 1rem;
        border-radius: 0.5rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

class AIEnhancedEchoAnalyzer:
    def __init__(self):
        # åŸºç¡€å…³é”®è¯åº“ï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
        self.sentiment_keywords = {
            'positive': ['å¥½', 'æ£’', 'ä¸é”™', 'å–œæ¬¢', 'æ»¡æ„', 'ä¼˜ç§€', 'å®Œç¾', 'æ¨è', 'èµ', 'çˆ±äº†', 'ç»™åŠ›', 'å¤ªæ£’äº†', 'æµç•…', 'æ–¹ä¾¿', 'ç®€å•', 'ç¾è§‚'],
            'negative': ['å·®', 'çƒ‚', 'åƒåœ¾', 'bug', 'å¡', 'é—ªé€€', 'æ…¢', 'éš¾ç”¨', 'å¤±æœ›', 'ç³Ÿç³•', 'é—®é¢˜', 'æ•…éšœ', 'å´©æºƒ', 'å¤æ‚', 'éº»çƒ¦', 'ä¸‘'],
            'neutral': ['ä¸€èˆ¬', 'è¿˜è¡Œ', 'æ™®é€š', 'å‡‘åˆ', 'ä¸­ç­‰', 'æ­£å¸¸']
        }
        
        self.intent_keywords = {
            'bugåé¦ˆ': ['bug', 'é”™è¯¯', 'æ•…éšœ', 'å´©æºƒ', 'é—ªé€€', 'å¡é¡¿', 'æ­»æœº', 'æ— å“åº”', 'å¼‚å¸¸', 'é—®é¢˜', 'ä¸èƒ½ç”¨', 'å¤±è´¥'],
            'åŠŸèƒ½å»ºè®®': ['å»ºè®®', 'å¸Œæœ›', 'æœŸå¾…', 'å¢åŠ ', 'æ·»åŠ ', 'æ”¹è¿›', 'ä¼˜åŒ–', 'æ›´æ–°', 'æ–°åŠŸèƒ½', 'å‡çº§', 'å®Œå–„'],
            'ä½“éªŒèµæ‰¬': ['å¥½ç”¨', 'æ–¹ä¾¿', 'ç®€å•', 'æµç•…', 'ç¾è§‚', 'å–œæ¬¢', 'æ»¡æ„', 'æ¨è', 'æ£’', 'ä¸é”™', 'å®Œç¾'],
            'ä½“éªŒæŠ±æ€¨': ['éš¾ç”¨', 'å¤æ‚', 'éº»çƒ¦', 'ä¸æ–¹ä¾¿', 'è®¾è®¡å·®', 'ç•Œé¢ä¸‘', 'ä½“éªŒå·®', 'ä¸å¥½ç”¨'],
            'å’¨è¯¢': ['æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'ä»€ä¹ˆæ—¶å€™', 'å“ªé‡Œ', 'è¯¢é—®', 'è¯·é—®', 'å’¨è¯¢']
        }
        
        self.high_freq_words = ['åŠŸèƒ½', 'ç•Œé¢', 'è®¾è®¡', 'ä½“éªŒ', 'é€Ÿåº¦', 'æ€§èƒ½', 'æ›´æ–°', 'ç‰ˆæœ¬', 'é—®é¢˜', 'å»ºè®®', 'å¸Œæœ›', 'å¥½ç”¨', 'æ–¹ä¾¿']

    def clean_text(self, text):
        """æ¸…ç†å’Œé¢„å¤„ç†æ–‡æœ¬æ•°æ®"""
        # å»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\.\!\?\,\;\:\"\'()ï¼ˆï¼‰ã€‚ï¼ï¼Ÿï¼Œï¼›ï¼š""'']', '', text)
        text = re.sub(r'\s+', ' ', text.strip())
        
        # åˆ†å‰²æˆè¯„è®ºåˆ—è¡¨
        comments = []
        for line in text.split('\n'):
            line = line.strip()
            if line and len(line) > 3:  # è¿‡æ»¤è¿‡çŸ­çš„è¯„è®º
                comments.append(line)
        
        return comments

    def advanced_sentiment_analysis(self, comments):
        """é«˜çº§æƒ…æ„Ÿåˆ†æ"""
        results = {'positive': 0, 'negative': 0, 'neutral': 0}
        detailed_results = []
        confidence_scores = []
        
        for comment in comments:
            comment_lower = comment.lower()
            
            # è®¡ç®—æƒ…æ„Ÿå¾—åˆ†
            pos_score = 0
            neg_score = 0
            
            # åŸºäºå…³é”®è¯çš„æƒ…æ„Ÿå¾—åˆ†
            for word in self.sentiment_keywords['positive']:
                if word in comment_lower:
                    pos_score += 1
                    if word in ['å®Œç¾', 'å¤ªæ£’äº†', 'çˆ±äº†']:
                        pos_score += 1  # å¼ºæƒ…æ„Ÿè¯é¢å¤–åŠ åˆ†
            
            for word in self.sentiment_keywords['negative']:
                if word in comment_lower:
                    neg_score += 1
                    if word in ['åƒåœ¾', 'ç³Ÿç³•', 'å´©æºƒ']:
                        neg_score += 1  # å¼ºæƒ…æ„Ÿè¯é¢å¤–åŠ åˆ†
            
            # è€ƒè™‘å¦å®šè¯çš„å½±å“
            negation_words = ['ä¸', 'æ²¡', 'æ— ', 'é', 'åˆ«', 'è«']
            for neg_word in negation_words:
                if neg_word in comment:
                    # å¦‚æœå¦å®šè¯åé¢è·Ÿç€ç§¯æè¯ï¼Œåˆ™å‡å°‘ç§¯æåˆ†æ•°
                    for pos_word in self.sentiment_keywords['positive']:
                        if f"{neg_word}{pos_word}" in comment or f"{neg_word}.*{pos_word}" in comment:
                            pos_score = max(0, pos_score - 1)
                    # å¦‚æœå¦å®šè¯åé¢è·Ÿç€æ¶ˆæè¯ï¼Œåˆ™å¢åŠ ç§¯æå€¾å‘
                    for neg_word_content in self.sentiment_keywords['negative']:
                        if f"{neg_word}{neg_word_content}" in comment or f"{neg_word}.*{neg_word_content}" in comment:
                            neg_score = max(0, neg_score - 1)
                            pos_score += 0.5
            
            # è®¡ç®—ç½®ä¿¡åº¦
            total_score = pos_score + neg_score
            confidence = min(total_score / 3.0, 1.0)  # å½’ä¸€åŒ–åˆ°0-1
            
            # ç¡®å®šæƒ…æ„Ÿ
            if pos_score > neg_score:
                sentiment = 'positive'
            elif neg_score > pos_score:
                sentiment = 'negative'
            else:
                sentiment = 'neutral'
            
            results[sentiment] += 1
            detailed_results.append({
                'comment': comment,
                'sentiment': sentiment,
                'confidence': confidence,
                'pos_score': pos_score,
                'neg_score': neg_score
            })
            confidence_scores.append(confidence)
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = np.mean(confidence_scores) if confidence_scores else 0
        
        return results, detailed_results, avg_confidence

    def smart_keyword_extraction(self, comments):
        """æ™ºèƒ½å…³é”®è¯æå–"""
        all_text = ' '.join(comments)
        
        # ä¸­æ–‡è¯è¯­æå–ï¼ˆç®€åŒ–ç‰ˆåˆ†è¯ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,4}', all_text)
        english_words = re.findall(r'[a-zA-Z]{3,10}', all_text.lower())
        
        # åœç”¨è¯è¿‡æ»¤
        stop_words = {
            'çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'è¿™', 'é‚£', 'å’Œ', 'ä¸', 'æˆ–', 
            'ä½†', 'ä¹Ÿ', 'éƒ½', 'å¾ˆ', 'éå¸¸', 'å°±', 'åœ¨', 'æœ‰', 'ä¼š', 'èƒ½', 'å¯ä»¥', 'ä¸æ˜¯', 
            'æ²¡æœ‰', 'æ¯”è¾ƒ', 'è§‰å¾—', 'æ„Ÿè§‰', 'è®¤ä¸º', 'åº”è¯¥', 'å¯èƒ½', 'ä¸€ä¸ª', 'è¿™ä¸ª', 'é‚£ä¸ª',
            'ç°åœ¨', 'ä»¥å‰', 'ä»¥å', 'æ—¶å€™', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'ç¡®å®', 'çœŸçš„', 'è¿˜æ˜¯'
        }
        
        # è¿‡æ»¤å¹¶ç»Ÿè®¡è¯é¢‘
        filtered_chinese = [word for word in chinese_words if word not in stop_words and len(word) >= 2]
        filtered_english = [word for word in english_words if word not in ['app', 'the', 'and', 'that', 'this']]
        
        all_words = filtered_chinese + filtered_english
        word_counts = Counter(all_words)
        
        # å¢åŠ é«˜é¢‘ä¸šåŠ¡è¯æƒé‡
        for word in self.high_freq_words:
            if word in word_counts:
                word_counts[word] = int(word_counts[word] * 1.5)
        
        return word_counts.most_common(25)

    def advanced_intent_classification(self, comments):
        """é«˜çº§æ„å›¾åˆ†ç±»"""
        results = {intent: 0 for intent in self.intent_keywords.keys()}
        detailed_results = []
        
        for comment in comments:
            comment_lower = comment.lower()
            intent_scores = {intent: 0 for intent in self.intent_keywords.keys()}
            
            # åŸºäºå…³é”®è¯åŒ¹é…è®¡ç®—å„æ„å›¾å¾—åˆ†
            for intent, keywords in self.intent_keywords.items():
                for keyword in keywords:
                    if keyword in comment_lower:
                        intent_scores[intent] += 1
                        
                        # ç‰¹æ®Šè§„åˆ™å¢å¼º
                        if intent == 'bugåé¦ˆ' and any(severity in comment_lower for severity in ['ä¸¥é‡', 'æ— æ³•', 'ä¸èƒ½', 'å´©æºƒ', 'é—ªé€€']):
                            intent_scores[intent] += 2
                        elif intent == 'åŠŸèƒ½å»ºè®®' and any(suggestion in comment_lower for suggestion in ['å¸Œæœ›', 'å»ºè®®', 'æœŸå¾…']):
                            intent_scores[intent] += 1
            
            # å¥å¼æ¨¡å¼è¯†åˆ«
            if re.search(r'[å¸ŒæœŸ]æœ›|å»ºè®®|å¯ä»¥.*å¢åŠ |èƒ½.*æ·»åŠ ', comment):
                intent_scores['åŠŸèƒ½å»ºè®®'] += 2
            if re.search(r'ä¸ºä»€ä¹ˆ|æ€ä¹ˆ.*ä¸|å¦‚ä½•.*è§£å†³', comment):
                intent_scores['å’¨è¯¢'] += 2
            if re.search(r'å¤ª.*äº†|éå¸¸.*|å¾ˆ.*', comment):
                if any(pos in comment for pos in self.sentiment_keywords['positive']):
                    intent_scores['ä½“éªŒèµæ‰¬'] += 1
                elif any(neg in comment for neg in self.sentiment_keywords['negative']):
                    intent_scores['ä½“éªŒæŠ±æ€¨'] += 1
            
            # ç¡®å®šä¸»è¦æ„å›¾
            best_intent = max(intent_scores, key=intent_scores.get)
            if intent_scores[best_intent] > 0:
                results[best_intent] += 1
                classified_intent = best_intent
            else:
                # åŸºäºæ•´ä½“æƒ…æ„Ÿè¿›è¡Œå…œåº•åˆ†ç±»
                if any(pos in comment_lower for pos in self.sentiment_keywords['positive']):
                    results['ä½“éªŒèµæ‰¬'] += 1
                    classified_intent = 'ä½“éªŒèµæ‰¬'
                elif any(neg in comment_lower for neg in self.sentiment_keywords['negative']):
                    results['ä½“éªŒæŠ±æ€¨'] += 1
                    classified_intent = 'ä½“éªŒæŠ±æ€¨'
                else:
                    if 'å…¶ä»–' not in results:
                        results['å…¶ä»–'] = 0
                    results['å…¶ä»–'] += 1
                    classified_intent = 'å…¶ä»–'
            
            detailed_results.append({
                'comment': comment,
                'intent': classified_intent,
                'scores': intent_scores
            })
        
        return results, detailed_results

    def generate_ai_insights(self, sentiment_details, intent_details, keywords):
        """ç”ŸæˆAIæ´å¯Ÿå’Œæ‘˜è¦"""
        total_comments = len(sentiment_details)
        
        # æå–ä¸åŒç±»å‹çš„ä»£è¡¨æ€§è¯„è®º
        positive_comments = [item for item in sentiment_details if item['sentiment'] == 'positive']
        negative_comments = [item for item in sentiment_details if item['sentiment'] == 'negative']
        
        # æŒ‰ç½®ä¿¡åº¦æ’åºé€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„è¯„è®º
        positive_comments.sort(key=lambda x: x['confidence'], reverse=True)
        negative_comments.sort(key=lambda x: x['confidence'], reverse=True)
        
        # ç”Ÿæˆé—®é¢˜ä¼˜å…ˆçº§
        priority_issues = []
        bug_comments = [item for item in intent_details if item['intent'] == 'bugåé¦ˆ']
        if len(bug_comments) > total_comments * 0.2:  # å¦‚æœbugåé¦ˆè¶…è¿‡20%
            priority_issues.append({
                'type': 'æŠ€æœ¯é—®é¢˜',
                'severity': 'é«˜',
                'count': len(bug_comments),
                'description': 'Bugåé¦ˆæ•°é‡è¾ƒå¤šï¼Œéœ€è¦ä¼˜å…ˆå¤„ç†æŠ€æœ¯é—®é¢˜'
            })
        
        complaint_comments = [item for item in intent_details if item['intent'] == 'ä½“éªŒæŠ±æ€¨']
        if len(complaint_comments) > total_comments * 0.15:  # å¦‚æœä½“éªŒæŠ±æ€¨è¶…è¿‡15%
            priority_issues.append({
                'type': 'ç”¨æˆ·ä½“éªŒ',
                'severity': 'ä¸­',
                'count': len(complaint_comments),
                'description': 'ç”¨æˆ·ä½“éªŒé—®é¢˜éœ€è¦å…³æ³¨å’Œæ”¹è¿›'
            })
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        suggestions = []
        suggestion_comments = [item for item in intent_details if item['intent'] == 'åŠŸèƒ½å»ºè®®']
        if suggestion_comments:
            suggestions.append(f"æ”¶åˆ°äº†{len(suggestion_comments)}æ¡åŠŸèƒ½å»ºè®®ï¼Œå¯è€ƒè™‘çº³å…¥äº§å“è§„åˆ’")
        
        if positive_comments:
            top_positive = positive_comments[0]['comment']
            suggestions.append(f"ç”¨æˆ·æœ€è®¤å¯çš„æ–¹é¢ï¼š{top_positive[:30]}...")
        
        # å…³é”®è¯è¶‹åŠ¿åˆ†æ
        tech_keywords = [kw for kw, count in keywords if kw in ['bug', 'å¡é¡¿', 'é—ªé€€', 'å´©æºƒ', 'æ•…éšœ']]
        ux_keywords = [kw for kw, count in keywords if kw in ['ç•Œé¢', 'è®¾è®¡', 'ä½“éªŒ', 'æ“ä½œ', 'åŠŸèƒ½']]
        
        keyword_insights = {
            'tech_focus': len(tech_keywords) > 0,
            'ux_focus': len(ux_keywords) > 0,
            'top_concerns': [kw for kw, count in keywords[:5]]
        }
        
        return {
            'positive_highlights': [item['comment'] for item in positive_comments[:3]],
            'negative_highlights': [item['comment'] for item in negative_comments[:3]],
            'priority_issues': priority_issues,
            'suggestions': suggestions,
            'keyword_insights': keyword_insights,
            'total_comments': total_comments,
            'confidence_score': np.mean([item['confidence'] for item in sentiment_details])
        }

def create_enhanced_visualizations(results):
    """åˆ›å»ºå¢å¼ºç‰ˆå¯è§†åŒ–å›¾è¡¨"""
    
    # 1. æƒ…æ„Ÿåˆ†å¸ƒå¸¦ç½®ä¿¡åº¦çš„é¥¼å›¾
    sentiment_values = list(results['sentiment_results'].values())
    sentiment_labels = ['æ­£é¢è¯„ä»·', 'è´Ÿé¢è¯„ä»·', 'ä¸­æ€§è¯„ä»·']
    
    # ç¡®ä¿æœ‰æœ‰æ•ˆæ•°æ®
    if sum(sentiment_values) == 0:
        sentiment_values = [1, 0, 0]  # é»˜è®¤å€¼é¿å…ç©ºå›¾è¡¨
    
    sentiment_fig = go.Figure(data=[go.Pie(
        labels=sentiment_labels,
        values=sentiment_values,
        hole=0.4,
        marker_colors=['#28a745', '#dc3545', '#ffc107'],
        textinfo='label+percent+value',
        textposition='auto',
        hovertemplate='<b>%{label}</b><br>æ•°é‡: %{value}<br>å æ¯”: %{percent}<extra></extra>'
    )])
    
    sentiment_fig.update_layout(
        title={
            'text': 'ç”¨æˆ·æƒ…æ„Ÿåˆ†å¸ƒåˆ†æ',
            'x': 0.5,
            'xanchor': 'center',
            'font': {'size': 18, 'family': 'Arial'}
        },
        font=dict(size=12),
        showlegend=True,
        legend=dict(orientation="v", yanchor="middle", y=0.5, xanchor="left", x=1.01)
    )
    
    # 2. æ„å›¾åˆ†ç±»æ¡å½¢å›¾ï¼ˆæ›¿æ¢ç€‘å¸ƒå›¾é¿å…å…¼å®¹æ€§é—®é¢˜ï¼‰
    intent_data = {k: v for k, v in results['intent_results'].items() if v > 0}
    
    # ç¡®ä¿æœ‰æœ‰æ•ˆæ•°æ®
    if not intent_data:
        intent_data = {'å…¶ä»–': 1}  # é»˜è®¤å€¼é¿å…ç©ºå›¾è¡¨
    
    # ä½¿ç”¨æ¡å½¢å›¾æ›¿ä»£ç€‘å¸ƒå›¾
    intent_fig = go.Figure(data=[go.Bar(
        x=list(intent_data.keys()),
        y=list(intent_data.values()),
        marker_color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'][:len(intent_data)],
        text=[f"{v}æ¡" for v in intent_data.values()],
        textposition='auto'
    )])
    
    intent_fig.update_layout(
        title={
            'text': 'ç”¨æˆ·åé¦ˆæ„å›¾åˆ†ç±»ç»Ÿè®¡',
            'x': 0.5,
            'xanchor': 'center',
            'font': {'size': 18, 'family': 'Arial'}
        },
        xaxis_title="åé¦ˆç±»å‹",
        yaxis_title="è¯„è®ºæ•°é‡",
        showlegend=False,
        xaxis=dict(tickangle=45)  # å€¾æ–œæ ‡ç­¾é¿å…é‡å 
    )
    
    return sentiment_fig, intent_fig

def main():
    # ä¸»æ ‡é¢˜
    st.markdown('<div class="main-header">ğŸ”Š ç”¨æˆ·ä¹‹å£°å›éŸ³å£ (Echo) Pro</div>', unsafe_allow_html=True)
    st.markdown('<div class="sub-header">ğŸ¤– AIé©±åŠ¨çš„ç”¨æˆ·åé¦ˆæ™ºèƒ½åˆ†æå¹³å°ï¼Œè®©æ¯ä¸ªå£°éŸ³éƒ½è¢«å¬è§ã€è¢«ç†è§£ã€è¢«åˆ†æ</div>', unsafe_allow_html=True)
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.markdown("### ğŸš€ Echo Pro ç‰¹è‰²åŠŸèƒ½")
        
        features = [
            {"icon": "ğŸ¯", "title": "é«˜ç²¾åº¦æƒ…æ„Ÿåˆ†æ", "desc": "æ™ºèƒ½è¯†åˆ«ç”¨æˆ·æƒ…æ„Ÿå€¾å‘"},
            {"icon": "ğŸ”", "title": "æ™ºèƒ½å…³é”®è¯æå–", "desc": "è‡ªåŠ¨å‘ç°ç”¨æˆ·å…³æ³¨ç„¦ç‚¹"},
            {"icon": "ğŸ“‹", "title": "æ„å›¾æ™ºèƒ½åˆ†ç±»", "desc": "ç²¾å‡†è¯†åˆ«ç”¨æˆ·åé¦ˆç±»å‹"},
            {"icon": "ğŸ“Š", "title": "å¯è§†åŒ–åˆ†ææŠ¥å‘Š", "desc": "ç›´è§‚å±•ç¤ºåˆ†æç»“æœ"},
            {"icon": "ğŸ’¡", "title": "AIæ™ºèƒ½æ´å¯Ÿ", "desc": "ç”Ÿæˆæ”¹è¿›å»ºè®®å’Œä¼˜å…ˆçº§"},
            {"icon": "ğŸ“", "title": "ä¸€é”®å¯¼å‡ºæŠ¥å‘Š", "desc": "æ”¯æŒå¤šæ ¼å¼æ•°æ®å¯¼å‡º"}
        ]
        
        for feature in features:
            st.markdown(f"""
            <div class="feature-card">
                <strong>{feature['icon']} {feature['title']}</strong><br>
                <small>{feature['desc']}</small>
            </div>
            """, unsafe_allow_html=True)
        
        st.markdown("---")
        st.markdown("### âš™ï¸ åˆ†æè®¾ç½®")
        
        # åˆ†æå‚æ•°è®¾ç½®
        confidence_threshold = st.slider("æƒ…æ„Ÿåˆ†æç½®ä¿¡åº¦é˜ˆå€¼", 0.0, 1.0, 0.6, 0.1)
        keyword_limit = st.slider("å…³é”®è¯æå–æ•°é‡", 10, 50, 25, 5)
        
        st.markdown("### ğŸ“ˆ ä½¿ç”¨ç»Ÿè®¡")
        if 'analysis_count' not in st.session_state:
            st.session_state.analysis_count = 0
        st.metric("æœ¬æ¬¡ä¼šè¯åˆ†ææ¬¡æ•°", st.session_state.analysis_count)

    # åˆå§‹åŒ–å¢å¼ºåˆ†æå™¨
    if 'enhanced_analyzer' not in st.session_state:
        st.session_state.enhanced_analyzer = AIEnhancedEchoAnalyzer()

    # æ•°æ®è¾“å…¥æ¨¡å—
    st.markdown("## ğŸ“¥ æ™ºèƒ½æ•°æ®è¾“å…¥æ¨¡å—")
    
    # è¾“å…¥æ–¹å¼é€‰æ‹©
    input_method = st.radio(
        "é€‰æ‹©è¾“å…¥æ–¹å¼ï¼š",
        ["æ–‡æœ¬ç²˜è´´", "æ–‡ä»¶ä¸Šä¼ ", "ç¤ºä¾‹æ•°æ®"],
        horizontal=True
    )
    
    user_input = ""
    
    if input_method == "æ–‡æœ¬ç²˜è´´":
        col1, col2 = st.columns([3, 1])
        with col1:
            user_input = st.text_area(
                "è¯·ç²˜è´´ç”¨æˆ·è¯„è®ºæ–‡æœ¬ï¼ˆæ¯è¡Œä¸€æ¡è¯„è®ºï¼‰ï¼š",
                height=250,
                placeholder="åœ¨æ­¤ç²˜è´´ä»App Storeã€Google Playã€ç¤¾äº¤åª’ä½“ç­‰å¹³å°æ”¶é›†çš„ç”¨æˆ·è¯„è®º...\n\nç¤ºä¾‹æ ¼å¼ï¼š\nè¿™ä¸ªAPPçœŸçš„å¾ˆå¥½ç”¨ï¼Œç•Œé¢è®¾è®¡å¾ˆç¾è§‚ï¼\næœ€è¿‘æ€»æ˜¯é—ªé€€ï¼Œå¸Œæœ›èƒ½ä¿®å¤ä¸€ä¸‹\nåŠŸèƒ½æŒºå…¨çš„ï¼Œå°±æ˜¯æœ‰ç‚¹å¡é¡¿"
            )
        
        with col2:
            st.markdown("### ğŸ“Š è¾“å…¥ç»Ÿè®¡")
            if user_input:
                lines = len([line for line in user_input.split('\n') if line.strip()])
                chars = len(user_input)
                words = len(user_input.split())
                st.metric("è¯„è®ºæ¡æ•°", lines)
                st.metric("å­—ç¬¦æ€»æ•°", chars)
                st.metric("è¯æ±‡æ•°é‡", words)
                
                # è¾“å…¥è´¨é‡è¯„ä¼°
                avg_length = chars / lines if lines > 0 else 0
                if avg_length > 20:
                    st.success("âœ… è¯„è®ºé•¿åº¦é€‚ä¸­")
                elif avg_length > 10:
                    st.warning("âš ï¸ è¯„è®ºåçŸ­")
                else:
                    st.error("âŒ è¯„è®ºè¿‡çŸ­")
    
    elif input_method == "ç¤ºä¾‹æ•°æ®":
        sample_data_options = {
            "ç”µå•†APPè¯„è®º": """è¿™æ¬¾è´­ç‰©APPç•Œé¢è®¾è®¡å¾ˆæ¼‚äº®ï¼Œæ“ä½œä¹Ÿå¾ˆæµç•…
å•†å“ç§ç±»ä¸°å¯Œï¼Œä»·æ ¼ä¹Ÿæ¯”è¾ƒä¼˜æƒ ï¼Œå¾ˆæ»¡æ„
æœ€è¿‘APPæ€»æ˜¯é—ªé€€ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¯ä»˜çš„æ—¶å€™ï¼Œå¾ˆå½±å“è´­ç‰©ä½“éªŒ
å¸Œæœ›èƒ½å¢åŠ å•†å“å¯¹æ¯”åŠŸèƒ½ï¼Œæ–¹ä¾¿é€‰æ‹©
å®¢æœå›å¤å¾ˆåŠæ—¶ï¼Œè§£å†³é—®é¢˜çš„æ•ˆç‡å¾ˆé«˜
ç‰©æµä¿¡æ¯æ›´æ–°ä¸åŠæ—¶ï¼Œä¸çŸ¥é“åŒ…è£¹åˆ°å“ªäº†
å»ºè®®ä¼˜åŒ–æœç´¢åŠŸèƒ½ï¼Œæœ‰æ—¶å€™æœä¸åˆ°æƒ³è¦çš„å•†å“
æ•´ä½“ä½“éªŒä¸é”™ï¼Œä¼šç»§ç»­ä½¿ç”¨å¹¶æ¨èç»™æœ‹å‹
ç»“è´¦é¡µé¢æœ‰bugï¼Œç‚¹å‡»æ”¯ä»˜æŒ‰é’®æ²¡æœ‰ååº”
å¸Œæœ›èƒ½æ”¯æŒæ›´å¤šæ”¯ä»˜æ–¹å¼ï¼Œæ¯”å¦‚æ•°å­—é’±åŒ…""",
            
            "ç¤¾äº¤åª’ä½“APPè¯„è®º": """ç•Œé¢ç®€æ´ç¾è§‚ï¼Œå¾ˆç¬¦åˆå¹´è½»äººçš„å®¡ç¾
å‘å¸ƒåŠŸèƒ½å¾ˆå®Œå–„ï¼Œæ”¯æŒå¤šç§æ ¼å¼çš„å†…å®¹
æœ€è¿‘æ›´æ–°åç»å¸¸å¡é¡¿ï¼Œå¸Œæœ›èƒ½ä¼˜åŒ–ä¸€ä¸‹æ€§èƒ½
å»ºè®®å¢åŠ å¤œé—´æ¨¡å¼ï¼Œé•¿æ—¶é—´ä½¿ç”¨çœ¼ç›ä¼šç´¯
ç§ä¿¡åŠŸèƒ½å¾ˆå¥½ç”¨ï¼Œå’Œæœ‹å‹èŠå¤©å¾ˆæ–¹ä¾¿
å¹¿å‘Šå¤ªå¤šäº†ï¼Œå½±å“ä½¿ç”¨ä½“éªŒ
å¸Œæœ›èƒ½å¢åŠ æ›´å¤šçš„æ»¤é•œå’Œè´´çº¸
ç”¨æˆ·éšç§ä¿æŠ¤åšå¾—ä¸é”™ï¼Œå¾ˆæœ‰å®‰å…¨æ„Ÿ
è§†é¢‘åŠ è½½é€Ÿåº¦æœ‰ç‚¹æ…¢ï¼Œç‰¹åˆ«æ˜¯åœ¨ç½‘ç»œä¸å¥½çš„æ—¶å€™
ç‚¹èµå’Œè¯„è®ºåŠŸèƒ½å“åº”åŠæ—¶ï¼Œäº’åŠ¨ä½“éªŒå¾ˆå¥½""",
            
            "å­¦ä¹ æ•™è‚²APPè¯„è®º": """è¯¾ç¨‹å†…å®¹è´¨é‡å¾ˆé«˜ï¼Œè€å¸ˆè®²è§£å¾ˆè¯¦ç»†
ç•Œé¢è®¾è®¡ç®€æ´ï¼ŒåŠŸèƒ½åˆ†å¸ƒåˆç†ï¼Œä½¿ç”¨æ–¹ä¾¿
è§†é¢‘æ’­æ”¾ç»å¸¸å¡é¡¿ï¼Œå½±å“å­¦ä¹ æ•ˆæœ
å¸Œæœ›èƒ½å¢åŠ ç¦»çº¿ä¸‹è½½åŠŸèƒ½ï¼Œæ–¹ä¾¿éšæ—¶å­¦ä¹ 
ç»ƒä¹ é¢˜ç›®è®¾è®¡å¾ˆå¥½ï¼Œæœ‰åŠ©äºå·©å›ºçŸ¥è¯†ç‚¹
å®¢æœæ€åº¦å¾ˆå¥½ï¼Œè§£ç­”é—®é¢˜å¾ˆè€å¿ƒ
å»ºè®®å¢åŠ å­¦ä¹ è¿›åº¦ç»Ÿè®¡åŠŸèƒ½
æ•´ä½“å­¦ä¹ ä½“éªŒä¸é”™ï¼Œç¡®å®æœ‰æå‡
APPå¯åŠ¨é€Ÿåº¦æœ‰ç‚¹æ…¢ï¼Œå¸Œæœ›èƒ½ä¼˜åŒ–
å¸Œæœ›èƒ½å¢åŠ æ›´å¤šäº’åŠ¨åŠŸèƒ½ï¼Œæé«˜å­¦ä¹ å…´è¶£"""
        }
        
        selected_sample = st.selectbox("é€‰æ‹©ç¤ºä¾‹æ•°æ®ï¼š", list(sample_data_options.keys()))
        user_input = sample_data_options[selected_sample]
        st.text_area("ç¤ºä¾‹æ•°æ®é¢„è§ˆï¼š", value=user_input, height=200, disabled=True)
    
    elif input_method == "æ–‡ä»¶ä¸Šä¼ ":
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ è¯„è®ºæ–‡ä»¶",
            type=['txt', 'csv'],
            help="æ”¯æŒ.txtå’Œ.csvæ ¼å¼æ–‡ä»¶ï¼Œæ¯è¡Œä¸€æ¡è¯„è®º"
        )
        if uploaded_file:
            try:
                content = uploaded_file.read().decode('utf-8')
                user_input = content
                st.success(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼å…±{len(content.split())}è¡Œå†…å®¹")
            except Exception as e:
                st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")

    # åˆ†ææ§åˆ¶é¢æ¿
    st.markdown("---")
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        analyze_button = st.button("ğŸš€ å¼€å§‹AIæ™ºèƒ½åˆ†æ", type="primary", use_container_width=True)
    
    with col2:
        if st.button("ğŸ§¹ æ¸…ç©ºè¾“å…¥", use_container_width=True):
            st.rerun()
    
    with col3:
        if st.button("ğŸ“Š æŸ¥çœ‹å†å²", use_container_width=True):
            st.info("å†å²åˆ†æåŠŸèƒ½å¼€å‘ä¸­...")

    # æ‰§è¡Œåˆ†æ
    if analyze_button and user_input:
        with st.spinner('ğŸ¤– AIæ­£åœ¨æ·±åº¦åˆ†æç”¨æˆ·åé¦ˆï¼Œè¯·ç¨å€™...'):
            # è¿›åº¦æ¡
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # æ­¥éª¤1ï¼šæ•°æ®é¢„å¤„ç†
            status_text.text('ğŸ“ æ­£åœ¨æ¸…ç†å’Œé¢„å¤„ç†æ–‡æœ¬æ•°æ®...')
            progress_bar.progress(20)
            time.sleep(0.5)
            
            comments = st.session_state.enhanced_analyzer.clean_text(user_input)
            
            if len(comments) == 0:
                st.error("âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„è¯„è®ºå†…å®¹ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ ¼å¼ã€‚")
                st.stop()
            
            # æ­¥éª¤2ï¼šæƒ…æ„Ÿåˆ†æ
            status_text.text('ğŸ­ æ­£åœ¨è¿›è¡Œé«˜ç²¾åº¦æƒ…æ„Ÿåˆ†æ...')
            progress_bar.progress(40)
            time.sleep(0.5)
            
            sentiment_results, sentiment_details, avg_confidence = st.session_state.enhanced_analyzer.advanced_sentiment_analysis(comments)
            
            # æ­¥éª¤3ï¼šå…³é”®è¯æå–
            status_text.text('ğŸ” æ­£åœ¨æå–å…³é”®è¯å’Œçƒ­ç‚¹è¯é¢˜...')
            progress_bar.progress(60)
            time.sleep(0.5)
            
            keywords = st.session_state.enhanced_analyzer.smart_keyword_extraction(comments)
            
            # æ­¥éª¤4ï¼šæ„å›¾åˆ†ç±»
            status_text.text('ğŸ“‹ æ­£åœ¨åˆ†æç”¨æˆ·åé¦ˆæ„å›¾...')
            progress_bar.progress(80)
            time.sleep(0.5)
            
            intent_results, intent_details = st.session_state.enhanced_analyzer.advanced_intent_classification(comments)
            
            # æ­¥éª¤5ï¼šç”Ÿæˆæ´å¯Ÿ
            status_text.text('ğŸ’¡ æ­£åœ¨ç”ŸæˆAIæ´å¯Ÿå’Œå»ºè®®...')
            progress_bar.progress(100)
            time.sleep(0.5)
            
            ai_insights = st.session_state.enhanced_analyzer.generate_ai_insights(
                sentiment_details, intent_details, keywords
            )
            
            # æ¸…é™¤è¿›åº¦æŒ‡ç¤ºå™¨
            progress_bar.empty()
            status_text.empty()
            
            # æ›´æ–°åˆ†æè®¡æ•°
            st.session_state.analysis_count += 1
            
            # å­˜å‚¨åˆ†æç»“æœ
            st.session_state.enhanced_results = {
                'sentiment_results': sentiment_results,
                'sentiment_details': sentiment_details,
                'keywords': keywords,
                'intent_results': intent_results,
                'intent_details': intent_details,
                'ai_insights': ai_insights,
                'avg_confidence': avg_confidence,
                'total_comments': len(comments),
                'analysis_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            st.success("ğŸ‰ åˆ†æå®Œæˆï¼è¯·æŸ¥çœ‹ä¸‹æ–¹è¯¦ç»†æŠ¥å‘Šã€‚")

    # æ˜¾ç¤ºå¢å¼ºåˆ†æç»“æœ
    if 'enhanced_results' in st.session_state:
        results = st.session_state.enhanced_results
        
        st.markdown("---")
        st.markdown("## ğŸ“Š AIæ™ºèƒ½åˆ†ææŠ¥å‘Š")
        
        # æ ¸å¿ƒæŒ‡æ ‡ä»ªè¡¨ç›˜
        st.markdown("### ğŸ¯ æ ¸å¿ƒæŒ‡æ ‡æ¦‚è§ˆ")
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            st.markdown(f"""
            <div class="metric-card">
                <h3>{results['total_comments']}</h3>
                <p>æ€»è¯„è®ºæ•°</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            positive_rate = round(results['sentiment_results']['positive'] / results['total_comments'] * 100, 1)
            st.markdown(f"""
            <div class="metric-card">
                <h3>{positive_rate}%</h3>
                <p>æ­£é¢è¯„ä»·ç‡</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            negative_rate = round(results['sentiment_results']['negative'] / results['total_comments'] * 100, 1)
            st.markdown(f"""
            <div class="metric-card">
                <h3>{negative_rate}%</h3>
                <p>è´Ÿé¢è¯„ä»·ç‡</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col4:
            confidence_score = round(results['avg_confidence'] * 100, 1)
            st.markdown(f"""
            <div class="metric-card">
                <h3>{confidence_score}%</h3>
                <p>åˆ†æç½®ä¿¡åº¦</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col5:
            intent_types = len([v for v in results['intent_results'].values() if v > 0])
            st.markdown(f"""
            <div class="metric-card">
                <h3>{intent_types}</h3>
                <p>åé¦ˆç±»å‹æ•°</p>
            </div>
            """, unsafe_allow_html=True)

        # å¢å¼ºç‰ˆå¯è§†åŒ–å›¾è¡¨
        st.markdown("### ğŸ“ˆ å¯è§†åŒ–åˆ†æå›¾è¡¨")
        
        sentiment_fig, intent_fig = create_enhanced_visualizations(results)
        
        col1, col2 = st.columns(2)
        with col1:
            st.plotly_chart(sentiment_fig, use_container_width=True)
        with col2:
            st.plotly_chart(intent_fig, use_container_width=True)
        
        # å…³é”®è¯åˆ†æ
        st.markdown("### â˜ï¸ æ™ºèƒ½å…³é”®è¯åˆ†æ")
        
        if results['keywords']:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # ç”Ÿæˆè¯äº‘
                keywords_dict = dict(results['keywords'])
                if keywords_dict:
                    try:
                        plt.figure(figsize=(12, 6))
                        font_path = os.path.join('echo-project', 'fonts', 'font.otf')
                        wordcloud = WordCloud(
                            font_path=font_path,
                            width=1000, 
                            height=500,
                            background_color='white',
                            colormap='plasma',
                            max_words=30,
                            relative_scaling=0.5,
                            collocations=False,
                            prefer_horizontal=0.7
                        ).generate_from_frequencies(keywords_dict)
                        
                        plt.imshow(wordcloud, interpolation='bilinear')
                        plt.axis('off')
                        plt.title('ç”¨æˆ·å…³æ³¨çƒ­ç‚¹è¯äº‘å›¾', fontsize=16, pad=20)
                        
                        img_buffer = io.BytesIO()
                        plt.savefig(img_buffer, format='png', bbox_inches='tight', dpi=200, facecolor='white')
                        img_buffer.seek(0)
                        
                        st.image(img_buffer, use_column_width=True)
                        plt.close()
                    except Exception as e:
                        st.warning("âš ï¸ è¯äº‘å›¾ç”Ÿæˆå¤±è´¥ï¼Œå¯èƒ½æ˜¯å­—ä½“é—®é¢˜ã€‚æ˜¾ç¤ºå…³é”®è¯åˆ—è¡¨ä½œä¸ºæ›¿ä»£ã€‚")
                        # å¤‡ç”¨æ˜¾ç¤ºæ–¹æ¡ˆ
                        keyword_text = " | ".join([f"{word}({count})" for word, count in results['keywords'][:20]])
                        st.markdown(f"**å…³é”®è¯**: {keyword_text}")
            
            with col2:
                st.markdown("#### ğŸ”¥ é«˜é¢‘å…³é”®è¯æ’è¡Œ")
                for i, (word, count) in enumerate(results['keywords'][:15], 1):
                    percentage = round(count / results['total_comments'] * 100, 1) if results['total_comments'] > 0 else 0
                    st.markdown(f"""
                    <div style="display: flex; justify-content: space-between; padding: 0.2rem 0; border-bottom: 1px solid #eee;">
                        <span><strong>{i}. {word}</strong></span>
                        <span style="color: #666;">{count}æ¬¡ ({percentage}%)</span>
                    </div>
                    """, unsafe_allow_html=True)

        # AIæ´å¯Ÿå’Œå»ºè®®
        st.markdown("### ğŸ¤– AIæ™ºèƒ½æ´å¯Ÿ")
        
        insights = results['ai_insights']
        
        # ä¼˜å…ˆçº§é—®é¢˜
        if insights['priority_issues']:
            st.markdown("#### âš ï¸ ä¼˜å…ˆå¤„ç†é—®é¢˜")
            for issue in insights['priority_issues']:
                severity_color = {'é«˜': '#dc3545', 'ä¸­': '#ffc107', 'ä½': '#28a745'}
                st.markdown(f"""
                <div style="background: linear-gradient(135deg, #fff3cd 0%, #ffeaa7 100%); 
                           padding: 1rem; border-radius: 0.5rem; border-left: 4px solid {severity_color[issue['severity']]}; margin: 0.5rem 0;">
                    <strong>ğŸš¨ {issue['type']} (ä¼˜å…ˆçº§: {issue['severity']})</strong><br>
                    ğŸ“Š å½±å“èŒƒå›´: {issue['count']}æ¡è¯„è®º<br>
                    ğŸ’¡ {issue['description']}
                </div>
                """, unsafe_allow_html=True)
        
        # æ ¸å¿ƒè§‚ç‚¹å±•ç¤º
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### ğŸ˜Š æ­£é¢åé¦ˆç²¾é€‰")
            for i, comment in enumerate(insights['positive_highlights'], 1):
                st.markdown(f"""
                <div class="insight-box positive-insight">
                    <strong>ğŸ’š ç”¨æˆ·å¥½è¯„ #{i}</strong><br>
                    "{comment}"
                </div>
                """, unsafe_allow_html=True)
        
        with col2:
            st.markdown("#### ğŸ˜Ÿ è´Ÿé¢åé¦ˆç²¾é€‰")
            for i, comment in enumerate(insights['negative_highlights'], 1):
                st.markdown(f"""
                <div class="insight-box negative-insight">
                    <strong>ğŸ’” ç”¨æˆ·å·®è¯„ #{i}</strong><br>
                    "{comment}"
                </div>
                """, unsafe_allow_html=True)
        
        # AIå»ºè®®
        if insights['suggestions']:
            st.markdown("#### ğŸ’¡ AIæ”¹è¿›å»ºè®®")
            for i, suggestion in enumerate(insights['suggestions'], 1):
                st.markdown(f"**{i}.** {suggestion}")
        
        # å…³é”®è¯æ´å¯Ÿ
        keyword_insights = insights['keyword_insights']
        if keyword_insights['tech_focus'] or keyword_insights['ux_focus']:
            st.markdown("#### ğŸ” å…³é”®è¯æ´å¯Ÿ")
            if keyword_insights['tech_focus']:
                st.warning("âš™ï¸ æŠ€æœ¯é—®é¢˜å…³æ³¨åº¦è¾ƒé«˜ï¼Œå»ºè®®ä¼˜å…ˆå¤„ç†æŠ€æœ¯å±‚é¢çš„bugå’Œæ€§èƒ½é—®é¢˜")
            if keyword_insights['ux_focus']:
                st.info("ğŸ¨ ç”¨æˆ·ä½“éªŒè®¨è®ºè¾ƒå¤šï¼Œå»ºè®®å…³æ³¨ç•Œé¢è®¾è®¡å’Œäº¤äº’ä¼˜åŒ–")
        
        # å¯¼å‡ºåŠŸèƒ½
        st.markdown("---")
        st.markdown("### ğŸ“ åˆ†ææŠ¥å‘Šå¯¼å‡º")
        
        # ç”Ÿæˆå®Œæ•´æŠ¥å‘Šæ•°æ®
        complete_report = {
            "åˆ†ææ¦‚è¦": {
                "åˆ†ææ—¶é—´": results['analysis_time'],
                "æ€»è¯„è®ºæ•°": results['total_comments'],
                "åˆ†æç½®ä¿¡åº¦": f"{round(results['avg_confidence'] * 100, 1)}%"
            },
            "æƒ…æ„Ÿåˆ†æç»“æœ": results['sentiment_results'],
            "æ„å›¾åˆ†ç±»ç»“æœ": results['intent_results'],
            "å…³é”®è¯åˆ†æ": {
                "é«˜é¢‘è¯æ±‡": dict(results['keywords'][:20])
            },
            "AIæ´å¯Ÿ": {
                "ä¼˜å…ˆé—®é¢˜": insights['priority_issues'],
                "æ”¹è¿›å»ºè®®": insights['suggestions'],
                "å…³é”®è¯æ´å¯Ÿ": insights['keyword_insights']
            },
            "ä»£è¡¨æ€§è¯„è®º": {
                "æ­£é¢åé¦ˆ": insights['positive_highlights'],
                "è´Ÿé¢åé¦ˆ": insights['negative_highlights']
            }
        }
        
        # ç”Ÿæˆè¯¦ç»†çš„CSVæ•°æ®
        detailed_data = []
        for detail in results['sentiment_details']:
            intent_info = next((item for item in results['intent_details'] 
                              if item['comment'] == detail['comment']), {'intent': 'æœªåˆ†ç±»'})
            detailed_data.append({
                'è¯„è®ºå†…å®¹': detail['comment'],
                'æƒ…æ„Ÿå€¾å‘': detail['sentiment'],
                'æƒ…æ„Ÿç½®ä¿¡åº¦': round(detail['confidence'], 3),
                'æ„å›¾åˆ†ç±»': intent_info['intent'],
                'æ­£é¢å¾—åˆ†': detail['pos_score'],
                'è´Ÿé¢å¾—åˆ†': detail['neg_score']
            })
        
        df_detailed = pd.DataFrame(detailed_data)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # JSONæŠ¥å‘Šä¸‹è½½
            report_json = json.dumps(complete_report, ensure_ascii=False, indent=2)
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½JSONåˆ†ææŠ¥å‘Š",
                data=report_json,
                file_name=f"echo_ai_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                use_container_width=True
            )
        
        with col2:
            # CSVè¯¦ç»†æ•°æ®ä¸‹è½½
            csv_data = df_detailed.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“Š ä¸‹è½½CSVè¯¦ç»†æ•°æ®",
                data=csv_data,
                file_name=f"echo_detailed_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv",
                use_container_width=True
            )
        
        with col3:
            # ç”Ÿæˆç®€è¦æ€»ç»“æŠ¥å‘Š
            summary_report = f"""
Echo AIåˆ†ææŠ¥å‘Šæ‘˜è¦
===================
åˆ†ææ—¶é—´: {results['analysis_time']}
æ€»è¯„è®ºæ•°: {results['total_comments']}æ¡
åˆ†æç½®ä¿¡åº¦: {round(results['avg_confidence'] * 100, 1)}%

æƒ…æ„Ÿåˆ†å¸ƒ:
â€¢ æ­£é¢è¯„ä»·: {results['sentiment_results']['positive']}æ¡ ({round(results['sentiment_results']['positive']/results['total_comments']*100, 1)}%)
â€¢ è´Ÿé¢è¯„ä»·: {results['sentiment_results']['negative']}æ¡ ({round(results['sentiment_results']['negative']/results['total_comments']*100, 1)}%)
â€¢ ä¸­æ€§è¯„ä»·: {results['sentiment_results']['neutral']}æ¡ ({round(results['sentiment_results']['neutral']/results['total_comments']*100, 1)}%)

ä¸»è¦åé¦ˆç±»å‹:
{chr(10).join([f"â€¢ {k}: {v}æ¡" for k, v in results['intent_results'].items() if v > 0])}

é«˜é¢‘å…³é”®è¯:
{', '.join([word for word, count in results['keywords'][:10]])}

ä¸»è¦å»ºè®®:
{chr(10).join([f"â€¢ {suggestion}" for suggestion in insights['suggestions']])}
            """
            
            st.download_button(
                label="ğŸ“„ ä¸‹è½½æ‘˜è¦æŠ¥å‘Š",
                data=summary_report,
                file_name=f"echo_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                mime="text/plain",
                use_container_width=True
            )
        
        # æ•°æ®è¡¨æ ¼å±•ç¤º
        st.markdown("### ğŸ“‹ è¯¦ç»†æ•°æ®è¡¨æ ¼")
        
        # æ•°æ®ç­›é€‰å™¨
        col1, col2, col3 = st.columns(3)
        with col1:
            sentiment_filter = st.multiselect(
                "ç­›é€‰æƒ…æ„Ÿç±»å‹:",
                ['positive', 'negative', 'neutral'],
                default=['positive', 'negative', 'neutral'],
                format_func=lambda x: {'positive': 'æ­£é¢', 'negative': 'è´Ÿé¢', 'neutral': 'ä¸­æ€§'}[x]
            )
        
        with col2:
            intent_options = list(set([item['intent'] for item in results['intent_details']]))
            intent_filter = st.multiselect(
                "ç­›é€‰æ„å›¾ç±»å‹:",
                intent_options,
                default=intent_options
            )
        
        with col3:
            confidence_min = st.slider("æœ€ä½ç½®ä¿¡åº¦:", 0.0, 1.0, 0.0, 0.1)
        
        # åº”ç”¨ç­›é€‰å™¨
        filtered_df = df_detailed[
            (df_detailed['æƒ…æ„Ÿå€¾å‘'].isin(sentiment_filter)) &
            (df_detailed['æ„å›¾åˆ†ç±»'].isin(intent_filter)) &
            (df_detailed['æƒ…æ„Ÿç½®ä¿¡åº¦'] >= confidence_min)
        ]
        
        st.dataframe(
            filtered_df,
            use_container_width=True,
            height=400,
            column_config={
                "è¯„è®ºå†…å®¹": st.column_config.TextColumn("è¯„è®ºå†…å®¹", width="large"),
                "æƒ…æ„Ÿå€¾å‘": st.column_config.TextColumn("æƒ…æ„Ÿå€¾å‘", width="small"),
                "æƒ…æ„Ÿç½®ä¿¡åº¦": st.column_config.ProgressColumn("ç½®ä¿¡åº¦", min_value=0, max_value=1),
                "æ„å›¾åˆ†ç±»": st.column_config.TextColumn("æ„å›¾åˆ†ç±»", width="medium"),
                "æ­£é¢å¾—åˆ†": st.column_config.NumberColumn("æ­£é¢å¾—åˆ†", format="%d"),
                "è´Ÿé¢å¾—åˆ†": st.column_config.NumberColumn("è´Ÿé¢å¾—åˆ†", format="%d")
            }
        )
        
        st.markdown(f"ğŸ“Š æ˜¾ç¤º {len(filtered_df)} / {len(df_detailed)} æ¡è®°å½•")
        
        # é«˜çº§åˆ†æåŠŸèƒ½
        st.markdown("---")
        st.markdown("### ğŸ”¬ é«˜çº§åˆ†æåŠŸèƒ½")
        
        tab1, tab2, tab3 = st.tabs(["ğŸ“ˆ è¶‹åŠ¿åˆ†æ", "ğŸ”— å…³è”åˆ†æ", "ğŸ¯ åˆ†ç¾¤åˆ†æ"])
        
        with tab1:
            st.markdown("#### æƒ…æ„Ÿè¶‹åŠ¿åˆ†æ")
            
            # æŒ‰è¯„è®ºé•¿åº¦åˆ†ææƒ…æ„Ÿåˆ†å¸ƒ
            df_detailed['è¯„è®ºé•¿åº¦'] = df_detailed['è¯„è®ºå†…å®¹'].str.len()
            length_bins = pd.cut(df_detailed['è¯„è®ºé•¿åº¦'], bins=5, labels=['å¾ˆçŸ­', 'è¾ƒçŸ­', 'ä¸­ç­‰', 'è¾ƒé•¿', 'å¾ˆé•¿'])
            df_detailed['é•¿åº¦åˆ†ç»„'] = length_bins
            
            length_sentiment = df_detailed.groupby(['é•¿åº¦åˆ†ç»„', 'æƒ…æ„Ÿå€¾å‘']).size().unstack(fill_value=0)
            all_sentiment_columns = ['positive', 'negative', 'neutral']
# 2. ä½¿ç”¨ reindex æ–¹æ³•ï¼Œè¡¥å…¨ç¼ºå¤±çš„åˆ—ï¼Œå¹¶ç”¨ 0 å¡«å……
            length_sentiment = length_sentiment.reindex(columns=all_sentiment_columns, fill_value=0)
            
            fig_trend = px.bar(
                x=length_sentiment.index,
                y=[length_sentiment['positive'], length_sentiment['negative'], length_sentiment['neutral']],
                title="ä¸åŒè¯„è®ºé•¿åº¦çš„æƒ…æ„Ÿåˆ†å¸ƒ",
                labels={'x': 'è¯„è®ºé•¿åº¦åˆ†ç»„', 'y': 'è¯„è®ºæ•°é‡'},
                color_discrete_map={'positive': '#28a745', 'negative': '#dc3545', 'neutral': '#ffc107'}
            )
            
            st.plotly_chart(fig_trend, use_container_width=True)
        
        with tab2:
            st.markdown("#### å…³é”®è¯å…³è”åˆ†æ")
            
            # åˆ†æå…³é”®è¯ä¸æƒ…æ„Ÿçš„å…³è”
            keyword_sentiment = {}
            for detail in results['sentiment_details']:
                comment = detail['comment'].lower()
                sentiment = detail['sentiment']
                
                for word, count in results['keywords'][:15]:
                    if word.lower() in comment:
                        if word not in keyword_sentiment:
                            keyword_sentiment[word] = {'positive': 0, 'negative': 0, 'neutral': 0}
                        keyword_sentiment[word][sentiment] += 1
            
            # åˆ›å»ºå…³è”çƒ­åŠ›å›¾æ•°æ®
            heatmap_data = []
            for word, sentiments in keyword_sentiment.items():
                total = sum(sentiments.values())
                if total > 0:
                    heatmap_data.append({
                        'å…³é”®è¯': word,
                        'æ­£é¢å…³è”åº¦': sentiments['positive'] / total,
                        'è´Ÿé¢å…³è”åº¦': sentiments['negative'] / total,
                        'ä¸­æ€§å…³è”åº¦': sentiments['neutral'] / total,
                        'æ€»å‡ºç°æ¬¡æ•°': total
                    })
            
            if heatmap_data:
                df_heatmap = pd.DataFrame(heatmap_data)
                df_heatmap = df_heatmap.sort_values('æ€»å‡ºç°æ¬¡æ•°', ascending=False).head(10)
                
                fig_heatmap = px.imshow(
                    df_heatmap[['æ­£é¢å…³è”åº¦', 'ä¸­æ€§å…³è”åº¦', 'è´Ÿé¢å…³è”åº¦']].T,
                    x=df_heatmap['å…³é”®è¯'],
                    y=['æ­£é¢å…³è”åº¦', 'ä¸­æ€§å…³è”åº¦', 'è´Ÿé¢å…³è”åº¦'],
                    color_continuous_scale='RdYlBu_r',
                    title="å…³é”®è¯æƒ…æ„Ÿå…³è”çƒ­åŠ›å›¾"
                )
                
                st.plotly_chart(fig_heatmap, use_container_width=True)
        
        with tab3:
            st.markdown("#### ç”¨æˆ·åˆ†ç¾¤åˆ†æ")
            
            # åŸºäºæƒ…æ„Ÿå’Œæ„å›¾è¿›è¡Œç”¨æˆ·åˆ†ç¾¤
            cluster_data = df_detailed.groupby(['æƒ…æ„Ÿå€¾å‘', 'æ„å›¾åˆ†ç±»']).size().reset_index(name='æ•°é‡')
            
            fig_cluster = px.sunburst(
                cluster_data,
                path=['æƒ…æ„Ÿå€¾å‘', 'æ„å›¾åˆ†ç±»'],
                values='æ•°é‡',
                title="ç”¨æˆ·åé¦ˆåˆ†ç¾¤æ—­æ—¥å›¾",
                color='æ•°é‡',
                color_continuous_scale='viridis'
            )
            
            st.plotly_chart(fig_cluster, use_container_width=True)
            
            # åˆ†ç¾¤ç‰¹å¾æè¿°
            st.markdown("##### ğŸ¯ åˆ†ç¾¤ç‰¹å¾åˆ†æ")
            
            major_clusters = cluster_data.nlargest(3, 'æ•°é‡')
            for idx, cluster in major_clusters.iterrows():
                st.markdown(f"""
                **ç¾¤ä½“ {idx+1}: {cluster['æƒ…æ„Ÿå€¾å‘']} + {cluster['æ„å›¾åˆ†ç±»']}**
                - è§„æ¨¡: {cluster['æ•°é‡']}æ¡è¯„è®º ({round(cluster['æ•°é‡']/results['total_comments']*100, 1)}%)
                - ç‰¹å¾: {'ç§¯æç”¨æˆ·ï¼Œå¯¹äº§å“ä½“éªŒæ»¡æ„' if cluster['æƒ…æ„Ÿå€¾å‘'] == 'positive' else 'æœ‰å¾…æ”¹å–„çš„ç”¨æˆ·ä½“éªŒ' if cluster['æƒ…æ„Ÿå€¾å‘'] == 'negative' else 'ä¸­æ€§ç”¨æˆ·ç¾¤ä½“'}
                """)

    # æ¯”è¾ƒåˆ†æåŠŸèƒ½
    if st.session_state.analysis_count > 1:
        st.markdown("---")
        st.markdown("### ğŸ“Š å†å²å¯¹æ¯”åˆ†æ")
        st.info("ğŸ’¡ å¤šæ¬¡åˆ†æåå¯ä»¥æŸ¥çœ‹è¶‹åŠ¿å˜åŒ–ï¼ˆåŠŸèƒ½è§„åˆ’ä¸­ï¼‰")

    # é¡µé¢åº•éƒ¨
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                color: white; padding: 2rem; border-radius: 1rem; margin: 2rem 0;'>
        <h3>ğŸš€ ç”¨æˆ·ä¹‹å£°å›éŸ³å£ (Echo) Pro</h3>
        <p style='margin: 0; font-size: 1.1rem;'>AIèµ‹èƒ½çš„ç”¨æˆ·åé¦ˆæ™ºèƒ½åˆ†æå¹³å° | è®©æ¯ä¸ªå£°éŸ³éƒ½è¢«å¬è§ã€è¢«ç†è§£ã€è¢«åˆ†æ</p>
        <small>Powered by Advanced AI Technology</small>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()